{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BinaryNet on CIFAR10 (Advanced)\n",
        "\n",
        "**Run this notebook here: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/plumerai/larq/master?filepath=examples%2Fbinarynet_advanced_cifar10.ipynb)**\n",
        "\n",
        "In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy of around 90% using a heavy lifting GPU like a Nvidia V100.\n",
        "On a Nvidia V100 it takes approximately 250 minutes to train. Compared to the original papers, [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/abs/1511.00363), and [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](http://arxiv.org/abs/1602.02830), we do not implement image whitening, but we use image augmentation, and a stepped learning rate scheduler."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import larq as lq\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import CIFAR10 Dataset\n",
        "\n",
        "Here we download the CIFAR10 dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = tf.keras.datasets.cifar10.load_data()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define our image augmentation technqiues, and create the dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_and_flip(image, labels, training):\n",
        "    image = tf.cast(image, tf.float32) / (255./2.) - 1.\n",
        "    if training:\n",
        "        image = tf.image.resize_image_with_crop_or_pad(image, 40, 40)\n",
        "        image = tf.random_crop(image, [32, 32, 3])\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "    return image, labels"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data, batch_size, training):\n",
        "    images, labels = data\n",
        "    labels = tf.one_hot(np.squeeze(labels), 10)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    dataset = dataset.repeat()\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.map(lambda x, y: resize_and_flip(x, y, training))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_dataset = create_dataset(train_data, batch_size, True)\n",
        "test_dataset = create_dataset(test_data, batch_size, False)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Binarynet\n",
        "\n",
        "Here we build the binarynet model layer by layer using a keras sequential model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# All quantized layers except the first will use the same options\n",
        "kwargs = dict(input_quantizer=\"ste_sign\",\n",
        "              kernel_quantizer=\"ste_sign\",\n",
        "              kernel_constraint=\"weight_clip\",\n",
        "              use_bias=False)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # In the first layer we only quantize the weights and not the input\n",
        "    lq.layers.QuantConv2D(128, 3,\n",
        "                          kernel_quantizer=\"ste_sign\",\n",
        "                          kernel_constraint=\"weight_clip\",\n",
        "                          use_bias=False,\n",
        "                          input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantConv2D(128, 3, padding=\"same\", **kwargs),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantConv2D(256, 3, padding=\"same\", **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantConv2D(256, 3, padding=\"same\", **kwargs),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantConv2D(512, 3, padding=\"same\", **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantConv2D(512, 3, padding=\"same\", **kwargs),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    lq.layers.QuantDense(1024, **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantDense(1024, **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    lq.layers.QuantDense(10, **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "    tf.keras.layers.Activation(\"softmax\")\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larq allows you to print a summary of the model that includes bit-precision information:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lq.models.summary(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer                     Outputs              # 1-bit    # 32-bit\n",
            "------------------------  -----------------  ---------  ----------\n",
            "quant_conv2d              (-1, 30, 30, 128)       3456           0\n",
            "batch_normalization_v1    (-1, 30, 30, 128)          0         384\n",
            "quant_conv2d_1            (-1, 30, 30, 128)     147456           0\n",
            "max_pooling2d             (-1, 15, 15, 128)          0           0\n",
            "batch_normalization_v1_1  (-1, 15, 15, 128)          0         384\n",
            "quant_conv2d_2            (-1, 15, 15, 256)     294912           0\n",
            "batch_normalization_v1_2  (-1, 15, 15, 256)          0         768\n",
            "quant_conv2d_3            (-1, 15, 15, 256)     589824           0\n",
            "max_pooling2d_1           (-1, 7, 7, 256)            0           0\n",
            "batch_normalization_v1_3  (-1, 7, 7, 256)            0         768\n",
            "quant_conv2d_4            (-1, 7, 7, 512)      1179648           0\n",
            "batch_normalization_v1_4  (-1, 7, 7, 512)            0        1536\n",
            "quant_conv2d_5            (-1, 7, 7, 512)      2359296           0\n",
            "max_pooling2d_2           (-1, 3, 3, 512)            0           0\n",
            "batch_normalization_v1_5  (-1, 3, 3, 512)            0        1536\n",
            "flatten                   (-1, 4608)                 0           0\n",
            "quant_dense               (-1, 1024)           4718592           0\n",
            "batch_normalization_v1_6  (-1, 1024)                 0        3072\n",
            "quant_dense_1             (-1, 1024)           1048576           0\n",
            "batch_normalization_v1_7  (-1, 1024)                 0        3072\n",
            "quant_dense_2             (-1, 10)               10240           0\n",
            "batch_normalization_v1_8  (-1, 10)                   0          30\n",
            "activation                (-1, 10)                   0           0\n",
            "Total                                         10352000       11550\n",
            "\n",
            "Total params: 10363550\n",
            "Trainable params: 10355850\n",
            "Non-trainable params: 7700\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "We compile and train the model as you are used to in Keras:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "initial_lr = 1e-3\n",
        "var_decay = 1e-5\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=initial_lr, decay=var_decay)\n",
        "model.compile(\n",
        "    optimizer=lq.optimizers.XavierLearningRateScaling(optimizer, model),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch):\n",
        "    return initial_lr * 0.1 ** (epoch // 100)\n",
        "\n",
        "trained_model = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=500,\n",
        "    steps_per_epoch=train_data[1].shape[0] // batch_size,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=test_data[1].shape[0] // batch_size,\n",
        "    verbose=1,\n",
        "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputExpanded": true,
        "scrolled": true
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}