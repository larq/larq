{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarynet CIFAR10 Example\n",
    "\n",
    "In this example we demonstrate how to use xquant to build binarynet with CIFAR10. Note that we only train for 10 epochs due to cost, so customise to your available resources/time! (On a Nvidia GTX1050ti MaxQ one epoch takes around 2 minutes, and on V100 one epoch takes 80 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the modules, noting that in this example we DO NOT use tensorflow_datasets. We use tensorflow, keras and xquant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import xquant as xq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download and normalize the CIFAR10 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_images = train_images.reshape((50000, 32, 32, 3))\n",
    "test_images = test_images.reshape((10000, 32, 32, 3))\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = (train_images / (255.0/2.0))-1., (test_images / (255.0/2.0))-1.\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impliment ZCA whitening as per original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/Work/xquant/env/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:334: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "datagen_train = ImageDataGenerator(zca_whitening=True)\n",
    "datagen_test = ImageDataGenerator(zca_whitening=True)\n",
    "datagen_train.fit(train_images)\n",
    "datagen_test.fit(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Binarynet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we build the binarynet model layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        xq.layers.QuantConv2D(\n",
    "            128,\n",
    "            3,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            use_bias=False,\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            input_shape=(32, 32, 3),\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantConv2D(\n",
    "            128,\n",
    "            3,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantConv2D(\n",
    "            256,\n",
    "            3,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantConv2D(\n",
    "            256,\n",
    "            3,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantConv2D(\n",
    "            512,\n",
    "            3,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantConv2D(\n",
    "            512,\n",
    "            3,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantDense(\n",
    "            1024,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantDense(\n",
    "            1024,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"hard_tanh\"),\n",
    "        xq.layers.QuantDense(\n",
    "            10,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "        tf.keras.layers.Activation(\"softmax\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally one can output a summary of the model. Commented out by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model. First we define some useful things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a learning rate with exponential decay and make it a 'Keras Callback'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_learning_rate(epoch):\n",
    "    LR_start = 0.001\n",
    "    LR_fin = 0.0000003\n",
    "    num_epochs = 500.\n",
    "    LR_decay = (LR_fin/LR_start)**(1./num_epochs)\n",
    "    lrate = LR_start*pow(LR_decay,epoch)\n",
    "    return lrate\n",
    "\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(exponential_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create a 'UsefulTracking' class such that we can make some plots later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsefulTracking(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.learning_rate_custom = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        #Optional learning rate print statement\n",
    "        #print(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        self.learning_rate_custom.append(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "\n",
    "useful_tracking=UsefulTracking()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    xq.optimizers.XavierLearningRateScaling(model,tf.keras.optimizers.Adam,lr=0.001,decay=0.01),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 162s 162ms/step - loss: 1.3990 - acc: 0.5428 - val_loss: 2.5922 - val_acc: 0.1586\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 157s 157ms/step - loss: 1.1573 - acc: 0.6543 - val_loss: 1.4856 - val_acc: 0.5161\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 160s 160ms/step - loss: 1.0999 - acc: 0.6779 - val_loss: 1.2188 - val_acc: 0.6284\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 159s 159ms/step - loss: 1.0675 - acc: 0.6927 - val_loss: 1.1602 - val_acc: 0.6558\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 157s 157ms/step - loss: 1.0511 - acc: 0.6993 - val_loss: 1.1261 - val_acc: 0.6648\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 159s 159ms/step - loss: 1.0389 - acc: 0.7057 - val_loss: 1.1092 - val_acc: 0.6730\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 162s 162ms/step - loss: 1.0214 - acc: 0.7129 - val_loss: 1.1037 - val_acc: 0.6723\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 163s 163ms/step - loss: 1.0171 - acc: 0.7151 - val_loss: 1.0863 - val_acc: 0.6787\n",
      "Epoch 9/50\n",
      "  32/1000 [..............................] - ETA: 2:02 - loss: 1.0000 - acc: 0.7281"
     ]
    }
   ],
   "source": [
    "#trained_model=model.fit(\n",
    "#    train_images, \n",
    "#    train_labels,\n",
    "#    batch_size=50, \n",
    "#    epochs=2,\n",
    "#    validation_data=(test_images, test_labels),\n",
    "#    shuffle=True\n",
    "#)\n",
    "\n",
    "trained_model=model.fit(\n",
    "    datagen_train.flow(train_images, train_labels,batch_size=50), \n",
    "    epochs=50,\n",
    "    validation_data=(datagen_test.flow(test_images, test_labels,batch_size=50)),\n",
    "    shuffle=True\n",
    "    #callbacks=[useful_tracking,lrate]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot a few useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trained_model.history['acc'])\n",
    "plt.plot(trained_model.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "print(np.max(trained_model.history['acc']))\n",
    "print(np.max(trained_model.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trained_model.history['loss'])\n",
    "plt.plot(trained_model.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "print(np.min(trained_model.history['loss']))\n",
    "print(np.min(trained_model.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(useful_tracking.learning_rate_custom)\n",
    "#plt.title('model learning rate')\n",
    "#plt.yscale('log')\n",
    "#plt.ylabel('learning rate')\n",
    "#plt.xlabel('epoch')\n",
    "#print(np.max(useful_tracking.learning_rate_custom))\n",
    "#print(np.min(useful_tracking.learning_rate_custom))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
